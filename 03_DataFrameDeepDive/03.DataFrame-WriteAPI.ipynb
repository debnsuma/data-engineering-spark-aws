{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2488e2eb",
   "metadata": {},
   "source": [
    "### DataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa605c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/flight-time.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489bf33",
   "metadata": {},
   "source": [
    "### Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1694b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_ddl = \"\"\"FL_DATE STRING, OP_CARRIER STRING, OP_CARRIER_FL_NUM INT, ORIGIN STRING, \n",
    "                ORIGIN_CITY_NAME STRING, DEST STRING, DEST_CITY_NAME STRING, CRS_DEP_TIME INT, DEP_TIME INT, \n",
    "                WHEELS_ON INT, TAXI_IN INT, CRS_ARR_TIME INT, ARR_TIME INT, CANCELLED STRING, DISTANCE INT\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c962c80",
   "metadata": {},
   "source": [
    "##### Beauty of Schema on Read, we can define the scheme at the time of reading the data. \n",
    "- `FL_DATE` => String \n",
    "- `CANCELLED` => String \n",
    "\n",
    "We can change the schema later on if we feel like "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60ec9b",
   "metadata": {},
   "source": [
    "### Reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9affb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read \\\n",
    "                .format(\"json\") \\\n",
    "                .schema(schema_ddl) \\\n",
    "                .option(\"mode\", \"FAILFAST\") \\\n",
    "                .option(\"dateFormat\", \"M/d/y\") \\\n",
    "                .load(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd3a42",
   "metadata": {},
   "source": [
    "### Fix the schema "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7300bc",
   "metadata": {},
   "source": [
    "Explore for functions available in PySpark. \n",
    "\n",
    "They are mostly available in 2 places :\n",
    "    \n",
    "- pyspark.sql.functions [link](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- Build-In Spark SQL functions [link](https://spark.apache.org/docs/latest/api/sql/index.html#if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f5bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "raw_df_2 = raw_df \\\n",
    "            .withColumn(\"FL_DATE\", F.to_date(\"FL_DATE\", \"M/d/y\")) \\\n",
    "            .withColumn(\"CANCELLED\", F.expr(\"if(CANCELLED==1, true, false)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c75ff",
   "metadata": {},
   "source": [
    "### Move the data back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56700a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_location = 's3://fcc-spark-example/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab22b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_df_2.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(processed_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b312be",
   "metadata": {},
   "source": [
    "Different `modes` : \n",
    "\n",
    "    - APPEND : Appends contents of the DF to existing data, creates new table if doesnt exisit \n",
    "    - OVERWRITE : Overwrite existing data, creates new table if doesnt exisit \n",
    "    - ERROR or ERRORIFEXISTS : Throws an exception if data already exists \n",
    "    - IGNORE : Write the data if the table doesnt exist, and ignore the operation if the data already exists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59deaaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-06 21:00:40          0 _SUCCESS\r\n",
      "2023-03-06 21:00:40    1829036 part-00000-03aaa650-3c6e-41d8-8696-1c8cac41187d-c000.snappy.parquet\r\n",
      "2023-03-06 21:00:40    1658258 part-00001-03aaa650-3c6e-41d8-8696-1c8cac41187d-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://fcc-spark-example/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7eac46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
