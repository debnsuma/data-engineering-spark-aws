{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7160619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-35.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f23d2c79d50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7e3ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-35.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469e304",
   "metadata": {},
   "source": [
    "# Higher Level APIs \n",
    "    - Dataframes \n",
    "    - Spark SQL \n",
    "    - Datasets -> Language specific (not available for Python, but available for Scala/Java) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d130c",
   "metadata": {},
   "source": [
    "### RDD\n",
    "- No schema \n",
    "- No metadata\n",
    "- Raw data distributed across different partitions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d14a93",
   "metadata": {},
   "source": [
    "### Table\n",
    "- Consists of `data` and `metadata` \n",
    "- Data is stored at storage layer (in the form of data files)\n",
    "- Metadata is stored in some metastore which holds the schema \n",
    "- When we run `select * from table` -> It gets the checks data and metadata together to give us the data in a tabular form\n",
    "- For example: `select color from table` -> If this col `color` is not present in the metastore/table metadata, it will throw an exception. In that case it will not even look at the data files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ccf8a",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "\n",
    "- It works in a similar manner \n",
    "- Data files (S3/HDFS/etc) + Metastore (some database, admins can decide, we dont have to worrk )\n",
    "- Metastore on AWS \n",
    "    - When you create tables in Glue, it stores metadata about those tables (like table names, column names, data types, etc.) in the AWS Glue Data Catalog, which serves as a centralized metastore for your AWS environment. Data Catalog is integrated with Amazon S3, Amazon RDS, Amazon Redshift, and other services.\n",
    "    - If you're using Apache Spark on Amazon EMR or other non-Glue AWS environments, you might choose to use the Glue Data Catalog as your metastore by configuring your Spark environment accordingly. The advantage of this is a unified metastore across multiple services and applications, all managed by AWS.\n",
    "    - This metadata is stored in a highly available and durable way, but the exact details of its storage are abstracted away from the user as part of the fully managed nature of AWS services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d55f2",
   "metadata": {},
   "source": [
    "### DataFrames and Spark SQL\n",
    "\n",
    "- Dataframes are nothing but RDD + Meradata (schema/structure)\n",
    "- `Spark Dataframes` are **not persistent (its in-memory)**\n",
    "    - Data - in-memory \n",
    "    - Metadata - in-memory \n",
    "    - There is no metastore, it is stored in a temp metadata catalog. Once the application is closed/stoped, its gone\n",
    "    - Dataframes is only visible to 1 session (our session where we create it)\n",
    "    - We can think of it as RDD with some structure\n",
    "    \n",
    "- `Spark Table` is always **persistent**\n",
    "    - After closing the session the data persists (data and metadata)\n",
    "    - Can be accessed via others across other sessions \n",
    "                        \n",
    "- Performance would be almost same whether we use Dataframe or Spark Table\n",
    "- They can be used interchangable, we can convert a Spark table to a Dataframe and vice-versa based on our requirement\n",
    "- Higher level APIs are more performant as Spark now knows about the metadata, and it can optimize the operation in a better and more efficient manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0620897f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-35.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f23d2c79d50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8dbd2",
   "metadata": {},
   "source": [
    "# Dataframe\n",
    "\n",
    "At its core, we do the following typically:\n",
    "\n",
    "    - Step 1: We load the data/some file and create a Spark DF \n",
    "    - Step 2: Perform some operation \n",
    "    - Step 3: Save/write the transformed data back to some storage (S3/HDFS/etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3ee9a",
   "metadata": {},
   "source": [
    "## Loading the data and creating a dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b8ff9",
   "metadata": {},
   "source": [
    "#### 1. CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b3a118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/diamonds.csv'\n",
    "\n",
    "df = (spark.read                               # reader API\n",
    "           .format('csv')                      # format is CSV\n",
    "           .option('header', 'true')           # consider first line as header \n",
    "           .option('inferSchema', 'true')      # infer the schema automatically\n",
    "           .load(data_set)                     # load the data \n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9669e8a",
   "metadata": {},
   "source": [
    "> It is not prefered to use inferSchema to infer the schema\n",
    "\n",
    ">    - It may not infer the schema correctly like data time column might get infered as string\n",
    ">    - it can lead to performance issues, as spark has to scan some data in oder to infer the schema\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e464e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|\n",
      "| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|\n",
      "| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|\n",
      "| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|\n",
      "|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|\n",
      "| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|\n",
      "|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|\n",
      "|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|\n",
      "|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|\n",
      "|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f3bf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Another way to read the data \n",
    "\n",
    "df = (spark\n",
    "        .read\n",
    "        .csv(data_set, header=True, inferSchema=True)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95140e8",
   "metadata": {},
   "source": [
    "#### 2. JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0db2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|    cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|    SI2|    E|  Ideal| 61.5|  326| 55.0|3.95|3.98|2.43|\n",
      "| 0.21|    SI1|    E|Premium| 59.8|  326| 61.0|3.89|3.84|2.31|\n",
      "| 0.23|    VS1|    E|   Good| 56.9|  327| 65.0|4.05|4.07|2.31|\n",
      "| 0.29|    VS2|    I|Premium| 62.4|  334| 58.0| 4.2|4.23|2.63|\n",
      "| 0.31|    SI2|    J|   Good| 63.3|  335| 58.0|4.34|4.35|2.75|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# meta-data is embeded within the data \n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/diamonds.json'\n",
    "\n",
    "df = (spark\n",
    "          .read\n",
    "          .json(data_set)\n",
    "      )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219d60bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: double (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- table: double (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb45876",
   "metadata": {},
   "source": [
    "#### 3. Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd2335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|      cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|  1.0|    SI1|    F|  Premium| 60.3| 5292| 58.0|6.47|6.43|3.89|\n",
      "|  0.4|    SI2|    G|     Good| 63.1|  596| 59.0|4.65| 4.7|2.95|\n",
      "| 0.54|    SI1|    I|    Ideal| 62.0| 1057| 55.0|5.21|5.25|3.24|\n",
      "| 1.01|    VS2|    F|Very Good| 59.4| 6288| 61.0|6.48|6.51|3.86|\n",
      "| 2.03|    SI2|    E|  Premium| 61.5|18477| 59.0|8.24|8.16|5.04|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# meta-data is embeded within the data \n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/diamonds_parquet' \n",
    "\n",
    "df = (spark.read\n",
    "      .parquet(data_set)\n",
    "      )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8310db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.repartition(4).write.format(\"parquet\").mode(\"overwrite\").save(\"s3://fcc-spark-example/dataset/diamonds_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89a27e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|      cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|  1.0|    SI1|    F|  Premium| 60.3| 5292| 58.0|6.47|6.43|3.89|\n",
      "|  0.4|    SI2|    G|     Good| 63.1|  596| 59.0|4.65| 4.7|2.95|\n",
      "| 0.54|    SI1|    I|    Ideal| 62.0| 1057| 55.0|5.21|5.25|3.24|\n",
      "| 1.01|    VS2|    F|Very Good| 59.4| 6288| 61.0|6.48|6.51|3.86|\n",
      "| 2.03|    SI2|    E|  Premium| 61.5|18477| 59.0|8.24|8.16|5.04|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb52bff",
   "metadata": {},
   "source": [
    "## Performing some transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bfb520",
   "metadata": {},
   "source": [
    "### Change the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669286e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+---------+-----+-----+-----+-----+-----+-----+\n",
      "|carat|clarity|color|      cut|depth|price|table|x_col|y_col|z_col|\n",
      "+-----+-------+-----+---------+-----+-----+-----+-----+-----+-----+\n",
      "|  1.0|    SI1|    F|  Premium| 60.3| 5292| 58.0| 6.47| 6.43| 3.89|\n",
      "|  0.4|    SI2|    G|     Good| 63.1|  596| 59.0| 4.65|  4.7| 2.95|\n",
      "| 0.54|    SI1|    I|    Ideal| 62.0| 1057| 55.0| 5.21| 5.25| 3.24|\n",
      "| 1.01|    VS2|    F|Very Good| 59.4| 6288| 61.0| 6.48| 6.51| 3.86|\n",
      "| 2.03|    SI2|    E|  Premium| 61.5|18477| 59.0| 8.24| 8.16| 5.04|\n",
      "| 0.32|   VVS1|    I|  Premium| 63.0|  756| 58.0| 4.38| 4.32| 2.74|\n",
      "|  0.9|    SI2|    E|     Good| 61.3| 3895| 61.0| 6.13| 6.17| 3.77|\n",
      "| 1.07|    VS2|    I|    Ideal| 60.6| 5167| 59.0| 6.64| 6.62| 4.02|\n",
      "| 1.51|    SI2|    G|  Premium| 62.4| 7695| 57.0| 7.35| 7.29| 4.57|\n",
      "| 0.31|    VS2|    F|Very Good| 63.0|  583| 57.0| 4.27| 4.33| 2.71|\n",
      "| 0.36|   VVS2|    F|    Ideal| 60.4|  853| 58.0| 4.61| 4.66|  2.8|\n",
      "| 0.55|    SI1|    F|     Good| 57.0| 1410| 62.0| 5.42| 5.44|  3.1|\n",
      "| 0.31|    VS2|    E|    Ideal| 61.0|  872| 54.0| 4.41| 4.37| 2.68|\n",
      "| 0.32|   VVS1|    F|    Ideal| 62.5|  854| 53.0|  4.4| 4.43| 2.76|\n",
      "| 0.31|    SI1|    E|Very Good| 63.2|  698| 59.0| 4.31| 4.24|  2.7|\n",
      "| 0.93|    VS1|    H|    Ideal| 61.8| 5375| 55.0| 6.28| 6.26| 3.88|\n",
      "| 0.71|    VS1|    D|  Premium| 62.9| 2860| 57.0| 5.66|  5.6| 3.54|\n",
      "| 0.51|    SI1|    E|Very Good| 63.2| 1443| 61.0| 5.08| 5.05|  3.2|\n",
      "| 2.08|    SI2|    E|    Ideal| 60.2|18128| 60.0| 8.28| 8.32|  5.0|\n",
      "|  0.9|    SI1|    E|  Premium| 61.3| 4209| 60.0| 6.19| 6.14| 3.78|\n",
      "+-----+-------+-----+---------+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the column name \n",
    "\n",
    "df2 = df.withColumnRenamed('x', 'x_col') \\\n",
    "       .withColumnRenamed('y', 'y_col') \\\n",
    "       .withColumnRenamed('z', 'z_col') \n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9150e5",
   "metadata": {},
   "source": [
    "### Performing some filter operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db470e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|    cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1.0|    SI1|    F|Premium| 60.3| 5292| 58.0|6.47|6.43|3.89|\n",
      "| 2.03|    SI2|    E|Premium| 61.5|18477| 59.0|8.24|8.16|5.04|\n",
      "| 0.32|   VVS1|    I|Premium| 63.0|  756| 58.0|4.38|4.32|2.74|\n",
      "| 1.51|    SI2|    G|Premium| 62.4| 7695| 57.0|7.35|7.29|4.57|\n",
      "| 0.71|    VS1|    D|Premium| 62.9| 2860| 57.0|5.66| 5.6|3.54|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Some filter operation \n",
    "\n",
    "df_premium = df.where(\"cut == 'Premium'\")\n",
    "df_premium.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209ce3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|    cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1.0|    SI1|    F|Premium| 60.3| 5292| 58.0|6.47|6.43|3.89|\n",
      "| 2.03|    SI2|    E|Premium| 61.5|18477| 59.0|8.24|8.16|5.04|\n",
      "| 0.32|   VVS1|    I|Premium| 63.0|  756| 58.0|4.38|4.32|2.74|\n",
      "| 1.51|    SI2|    G|Premium| 62.4| 7695| 57.0|7.35|7.29|4.57|\n",
      "| 0.71|    VS1|    D|Premium| 62.9| 2860| 57.0|5.66| 5.6|3.54|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where() is an alias for filter()\n",
    "\n",
    "df_premium = df.filter(\"cut == 'Premium'\")\n",
    "df_premium.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29251907",
   "metadata": {},
   "source": [
    "### Changing the datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8a84e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "\n",
    "df_orders =  (spark.read                               # reader API\n",
    "                   .format('csv')                      # format is CSV\n",
    "                   .option('header', 'true')           # consider first line as header \n",
    "                   .option('inferSchema', 'true')      # infer the schema automatically\n",
    "                   .load(data_set)                     # load the data \n",
    "             )\n",
    "\n",
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "437701f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cab6465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing the data type from Integer Type to Long Type\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "df_orders2 = df_orders.withColumn('order_customer_id', \n",
    "                                  df_orders['order_customer_id'].cast(T.LongType())\n",
    "                                 )\n",
    "\n",
    "df_orders2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c329938",
   "metadata": {},
   "source": [
    "#### Dataframe to Spark Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a33e7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|      cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|  1.0|    SI1|    F|  Premium| 60.3| 5292| 58.0|6.47|6.43|3.89|\n",
      "|  0.4|    SI2|    G|     Good| 63.1|  596| 59.0|4.65| 4.7|2.95|\n",
      "| 0.54|    SI1|    I|    Ideal| 62.0| 1057| 55.0|5.21|5.25|3.24|\n",
      "| 1.01|    VS2|    F|Very Good| 59.4| 6288| 61.0|6.48|6.51|3.86|\n",
      "| 2.03|    SI2|    E|  Premium| 61.5|18477| 59.0|8.24|8.16|5.04|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/diamonds_parquet' \n",
    "\n",
    "df = ( spark\n",
    "          .read\n",
    "          .parquet(data_set)\n",
    "      )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc86c97",
   "metadata": {},
   "source": [
    "#### Local Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31facb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('diamonds') \n",
    "\n",
    "# Now we have a distributed table/view called 'diamonds' in our Spark Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e9f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_premium = spark.sql('SELECT * \\\n",
    "                           FROM diamonds \\\n",
    "                           WHERE cut=\"Premium\" \\\n",
    "                        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "046279ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|    cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1.0|    SI1|    F|Premium| 60.3| 5292| 58.0|6.47|6.43|3.89|\n",
      "| 2.03|    SI2|    E|Premium| 61.5|18477| 59.0|8.24|8.16|5.04|\n",
      "| 0.32|   VVS1|    I|Premium| 63.0|  756| 58.0|4.38|4.32|2.74|\n",
      "| 1.51|    SI2|    G|Premium| 62.4| 7695| 57.0|7.35|7.29|4.57|\n",
      "| 0.71|    VS1|    D|Premium| 62.9| 2860| 57.0|5.66| 5.6|3.54|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_premium.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9945807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 16:23:33 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\n",
      "23/07/10 16:23:33 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "23/07/10 16:23:33 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='diamonds', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables() \n",
    "# After this open a new `pyspark` shell and run the same `spark.catalog.listTables()` \n",
    "# We will see no tables, as this table was created as a \"Local Table\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049c45f",
   "metadata": {},
   "source": [
    "#### Spark Table to Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b08256b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ( spark\n",
    "          .read\n",
    "          .table('diamonds')\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "badab60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|carat|clarity|color|      cut|depth|price|table|   x|   y|   z|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "|  1.0|    SI1|    F|  Premium| 60.3| 5292| 58.0|6.47|6.43|3.89|\n",
      "|  0.4|    SI2|    G|     Good| 63.1|  596| 59.0|4.65| 4.7|2.95|\n",
      "| 0.54|    SI1|    I|    Ideal| 62.0| 1057| 55.0|5.21|5.25|3.24|\n",
      "| 1.01|    VS2|    F|Very Good| 59.4| 6288| 61.0|6.48|6.51|3.86|\n",
      "| 2.03|    SI2|    E|  Premium| 61.5|18477| 59.0|8.24|8.16|5.04|\n",
      "+-----+-------+-----+---------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18972fb9",
   "metadata": {},
   "source": [
    "#### Global Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bce33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b19d964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='diamonds', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aeab0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView('diamonds')\n",
    "spark.catalog.dropTempView('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c06b640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356ceed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
