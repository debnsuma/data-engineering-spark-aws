{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca586d9",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31225c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596e682",
   "metadata": {},
   "source": [
    "### Check all the `databases` present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc7dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SHOW databases').show()           # This is coming from AWS Glue Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8d06b",
   "metadata": {},
   "source": [
    "### Check the present `database` selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase() # This with show the presently selected `database`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378aa65",
   "metadata": {},
   "source": [
    "### Check all the `tables` present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SHOW tables').show() # In the `default` Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713a66a",
   "metadata": {},
   "source": [
    "### Select a particular `database`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edd22f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('USE db_youtube_raw')   # Select a differet database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711da82-15be-48a0-ac2f-7af7cf8398ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8916203",
   "metadata": {},
   "source": [
    "### Show the `tables` within the `database`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61672b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SHOW tables').show(truncate=False)  # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26f729",
   "metadata": {},
   "source": [
    "### Create a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fdd74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS my_db_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0340fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff898ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('USE my_db_spark')              # Use this newly created database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718cc7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SHOW tables').show()           # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b9108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if a particular database is present or not (in case the list is long)\n",
    "\n",
    "( \n",
    "    spark\n",
    "    .sql('SHOW databases')\n",
    "    .filter(\"namespace = 'db_youtube_raw'\")\n",
    "    .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee6112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Another way to check if a list of databases are present or not (in case the list is long)\n",
    "\n",
    "( \n",
    "    spark\n",
    "    .sql('SHOW databases')\n",
    "    .filter(\"namespace LIKE 'db%'\")\n",
    "    .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0945d47",
   "metadata": {},
   "source": [
    "### Create a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4bd30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('USE my_db_spark') \n",
    "spark.catalog.currentDatabase()    # Check the present database (which is selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654cddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SHOW tables').show(truncate=False)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237697dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('CREATE TABLE orders \\\n",
    "               (order_id integer, \\\n",
    "                order_date string, \\\n",
    "                customer_id integer, \\\n",
    "                order_status string)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a94c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87a363",
   "metadata": {},
   "source": [
    "> Now you can go to a new shell and run this \n",
    "```python\n",
    "spark.sql('USE my_db_spark')\n",
    "spark.sql('SHOW tables').show()\n",
    "+-----------+---------+-----------+                                             \n",
    "|  namespace|tableName|isTemporary|\n",
    "+-----------+---------+-----------+\n",
    "|my_db_spark|   orders|      false|\n",
    "+-----------+---------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524f858",
   "metadata": {},
   "source": [
    "### Create a `TempView` using the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000f665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets first create a DF using that we will create a TempView\n",
    "dataset = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "df = spark.read.csv(dataset, header=True, inferSchema=True)\n",
    "\n",
    "# Creating a TempView\n",
    "df.createOrReplaceTempView('orders_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee60c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02caf82e",
   "metadata": {},
   "source": [
    "> Now you can go to a new shell and run this **(WE WON'T SEE THE TEMP VIEW)**\n",
    "```python\n",
    "spark.sql('USE my_db_spark')\n",
    "spark.sql('SHOW tables').show()\n",
    "+-----------+---------+-----------+                                             \n",
    "|  namespace|tableName|isTemporary|\n",
    "+-----------+---------+-----------+\n",
    "|my_db_spark|   orders|      false|\n",
    "+-----------+---------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa703bc",
   "metadata": {},
   "source": [
    "### Insert data from the `TempView` into the new `Table` (the persistent table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae30c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO orders \\\n",
    "            SELECT * \\\n",
    "            FROM orders_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c7af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM orders\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfb612",
   "metadata": {},
   "source": [
    "### Describe `Table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e5c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE TABLE orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2ad6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the persistent table \n",
    "spark.sql(\"DESCRIBE EXTENDED orders\").show(truncate=False)              # Its a managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e776d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the TempView\n",
    "spark.sql(\"DESCRIBE EXTENDED orders_view\").show()              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152fae5",
   "metadata": {},
   "source": [
    "### Check the underline data in `HDFS` (Managed Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c3234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f698e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65c290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "hadoop fs -head hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders/part-00000-e687636c-e1bf-4205-baef-0315bae3cc48-c000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0967b",
   "metadata": {},
   "source": [
    "### Deleting the `table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8c1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE orders\") # It actually deleted the metadata and the data (BOTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d184be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"DESCRIBE TABLE orders\").show() # It will throw an ERROR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ea37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders\n",
    "\n",
    "# It should give an error (Here we used MANAGED Table, and hence the data and metadata both got deleted when we ran DROP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f4956",
   "metadata": {},
   "source": [
    "# Clean Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969981d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP DATABASE my_db_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3efb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7d08a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('USE default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d606e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5d051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83bf3c-d859-4a59-b792-1a6ed88dfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c224c1-7d42-4ffe-986a-391a02163ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
