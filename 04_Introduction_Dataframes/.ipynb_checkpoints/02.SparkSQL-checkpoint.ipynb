{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca586d9",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31225c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596e682",
   "metadata": {},
   "source": [
    "### Check all the `databases` present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bc7dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:10:00 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\n",
      "23/07/10 17:10:00 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "23/07/10 17:10:01 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()           # This is coming from AWS Glue Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8d06b",
   "metadata": {},
   "source": [
    "### Check the present `database` selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c85bdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase() # This with show the presently selected `database`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378aa65",
   "metadata": {},
   "source": [
    "### Check all the `tables` present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786c98f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show() # In the `default` Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713a66a",
   "metadata": {},
   "source": [
    "### Select a particular `database`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8edd22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE dev_feedback')   # Select a differet database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8916203",
   "metadata": {},
   "source": [
    "### Show the `tables` within the `database`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61672b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()  # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26f729",
   "metadata": {},
   "source": [
    "### Create a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197fdd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:10:03 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS my_db_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0340fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "|         my_db_spark|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff898ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE my_db_spark') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "718cc7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()           # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ba08a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------+-----------+\n",
      "|namespace     |tableName                    |isTemporary|\n",
      "+--------------+-----------------------------+-----------+\n",
      "|db_youtube_raw|raw_statistics               |false      |\n",
      "|db_youtube_raw|raw_statistics_reference_data|false      |\n",
      "+--------------+-----------------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('USE db_youtube_raw')\n",
    "spark.sql('SHOW tables').show(truncate=False)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f900130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "|         my_db_spark|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()        # Other tables are coming from AWS Glue Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94b9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|my_db_spark|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if a particular database is present or not (in case the list is long)\n",
    "\n",
    "( \n",
    "    spark\n",
    "    .sql('SHOW databases')\n",
    "    .filter(\"namespace = 'my_db_spark'\")\n",
    "    .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdee6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if a list of databases are present or not (in case the list is long)\n",
    "\n",
    "( \n",
    "    spark\n",
    "    .sql('SHOW databases')\n",
    "    .filter(\"namespace LIKE 'db%'\")\n",
    "    .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0945d47",
   "metadata": {},
   "source": [
    "### Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d4bd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_db_spark'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE my_db_spark') \n",
    "spark.catalog.currentDatabase()    # Check the present database (which is selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "654cddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show(truncate=False)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "237697dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:10:09 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=80b63cd3-0979-414a-b58a-483422bcc5a3, clientType=HIVECLI]\n",
      "23/07/10 17:10:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/07/10 17:10:09 INFO AWSCatalogMetastoreClient: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "23/07/10 17:10:09 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n",
      "23/07/10 17:10:10 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n",
      "23/07/10 17:10:10 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE TABLE my_db_spark.orders \\\n",
    "               (order_id integer, \\\n",
    "                order_date string, \\\n",
    "                customer_id integer, \\\n",
    "                order_status string)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2b07a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_db_spark'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()    # Check the present database (which is selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c76a94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------+\n",
      "|  namespace|tableName|isTemporary|\n",
      "+-----------+---------+-----------+\n",
      "|my_db_spark|   orders|      false|\n",
      "+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87a363",
   "metadata": {},
   "source": [
    "> Now you can go to a new shell and run this \n",
    "```python\n",
    "spark.sql('USE my_db_spark')\n",
    "spark.sql('SHOW tables').show()\n",
    "+-----------+---------+-----------+                                             \n",
    "|  namespace|tableName|isTemporary|\n",
    "+-----------+---------+-----------+\n",
    "|my_db_spark|   orders|      false|\n",
    "+-----------+---------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524f858",
   "metadata": {},
   "source": [
    "### Create a `TempView` using the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0000f665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets first create a DF using that we will create a TempView\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "df = spark.read.csv('s3://fcc-spark-example/dataset/2023/orders.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Creating a TempView\n",
    "df.createOrReplaceTempView('my_db_spark.orders_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ee60c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|  namespace|  tableName|isTemporary|\n",
      "+-----------+-----------+-----------+\n",
      "|my_db_spark|     orders|      false|\n",
      "|           |orders_view|       true|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02caf82e",
   "metadata": {},
   "source": [
    "> Now you can go to a new shell and run this **(WE WON'T SEE THE TEMP VIEW)**\n",
    "```python\n",
    "spark.sql('USE my_db_spark')\n",
    "spark.sql('SHOW tables').show()\n",
    "+-----------+---------+-----------+                                             \n",
    "|  namespace|tableName|isTemporary|\n",
    "+-----------+---------+-----------+\n",
    "|my_db_spark|   orders|      false|\n",
    "+-----------+---------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa703bc",
   "metadata": {},
   "source": [
    "### Insert data from the `TempView` into the new `Table` (the persistent table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cae30c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:10:19 INFO log: Updating table stats fast for orders                \n",
      "23/07/10 17:10:19 INFO log: Updated size of table orders to 2862089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO orders \\\n",
    "            SELECT * \\\n",
    "            FROM orders_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b88c7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM orders\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfb612",
   "metadata": {},
   "source": [
    "### Describe `Table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7e5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|    order_id|      int|   null|\n",
      "|  order_date|   string|   null|\n",
      "| customer_id|      int|   null|\n",
      "|order_status|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebe2ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|            order_id|                 int|   null|\n",
      "|          order_date|              string|   null|\n",
      "|         customer_id|                 int|   null|\n",
      "|        order_status|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|         my_db_spark|       |\n",
      "|               Table|              orders|       |\n",
      "|               Owner|              hadoop|       |\n",
      "|        Created Time|Mon Jul 10 17:10:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|  Spark 3.3.0-amzn-1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|                hive|       |\n",
      "|          Statistics|       2862089 bytes|       |\n",
      "|            Location|hdfs://ip-172-31-...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the persistent table \n",
    "spark.sql(\"DESCRIBE EXTENDED orders\").show()              # Its a managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b07e776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|         order_id|      int|   null|\n",
      "|       order_date|timestamp|   null|\n",
      "|order_customer_id|      int|   null|\n",
      "|     order_status|   string|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the TempView\n",
    "spark.sql(\"DESCRIBE EXTENDED orders_view\").show()              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152fae5",
   "metadata": {},
   "source": [
    "### Check the underline data in `HDFS` (Managed Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "723c3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxrwxrwt   - hadoop spark          0 2023-07-10 17:10 hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c91f698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwxrwt   1 hadoop spark    2862089 2023-07-10 17:10 hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders/part-00000-a4c7eb3f-7c59-447c-8186-4c73de69baef-c000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d65c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\u00012013-07-25 00:00:00\u000111599\u0001CLOSED\n",
      "2\u00012013-07-25 00:00:00\u0001256\u0001PENDING_PAYMENT\n",
      "3\u00012013-07-25 00:00:00\u000112111\u0001COMPLETE\n",
      "4\u00012013-07-25 00:00:00\u00018827\u0001CLOSED\n",
      "5\u00012013-07-25 00:00:00\u000111318\u0001COMPLETE\n",
      "6\u00012013-07-25 00:00:00\u00017130\u0001COMPLETE\n",
      "7\u00012013-07-25 00:00:00\u00014530\u0001COMPLETE\n",
      "8\u00012013-07-25 00:00:00\u00012911\u0001PROCESSING\n",
      "9\u00012013-07-25 00:00:00\u00015657\u0001PENDING_PAYMENT\n",
      "10\u00012013-07-25 00:00:00\u00015648\u0001PENDING_PAYMENT\n",
      "11\u00012013-07-25 00:00:00\u0001918\u0001PAYMENT_REVIEW\n",
      "12\u00012013-07-25 00:00:00\u00011837\u0001CLOSED\n",
      "13\u00012013-07-25 00:00:00\u00019149\u0001PENDING_PAYMENT\n",
      "14\u00012013-07-25 00:00:00\u00019842\u0001PROCESSING\n",
      "15\u00012013-07-25 00:00:00\u00012568\u0001COMPLETE\n",
      "16\u00012013-07-25 00:00:00\u00017276\u0001PENDING_PAYMENT\n",
      "17\u00012013-07-25 00:00:00\u00012667\u0001COMPLETE\n",
      "18\u00012013-07-25 00:00:00\u00011205\u0001CLOSED\n",
      "19\u00012013-07-25 00:00:00\u00019488\u0001PENDING_PAYMENT\n",
      "20\u00012013-07-25 00:00:00\u00019198\u0001PROCESSING\n",
      "21\u00012013-07-25 00:00:00\u00012711\u0001PENDING\n",
      "22\u00012013-07-25 00:00:00\u0001333\u0001COMPLETE\n",
      "23\u00012013-07-25 00:00:00\u00014367\u0001PENDING_PAYMENT\n",
      "24\u00012013-07-25 00:00:00\u000111441\u0001CLOSED\n",
      "25\u00012013-07-25 00:00:00\u00019503\u0001CLOSED\n",
      "26\u00012013-07-25 00:00:00\u00017562\u0001COMPLETE\n",
      "27\u00012013-07-25 00:00:00"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "hadoop fs -head hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders/part-00000-a4c7eb3f-7c59-447c-8186-4c73de69baef-c000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0967b",
   "metadata": {},
   "source": [
    "### Deleting the `table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48f8c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:11:46 INFO GlueMetastoreClientDelegate: Initiating drop table partitions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8d184be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DESCRIBE TABLE orders\").show() # It will throw an ERROR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "352ea37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It should give an error (Here we used MANAGED Table, and hence the data and metadata both got deleted when we ran DROP)\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f4956",
   "metadata": {},
   "source": [
    "# Clean Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3969981d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE my_db_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0f3efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_db_spark'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd7d08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "997d606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dea5d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW databases\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
