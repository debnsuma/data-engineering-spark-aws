{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55570d8a",
   "metadata": {},
   "source": [
    "# So far we have see the following:\n",
    "    - How we can create a Database\n",
    "    - How we can create a Table\n",
    "    - How to load the data from a tempView to a Table \n",
    "    - We created Managed Table, and when we dropped the table:\n",
    "        - Both the data and metadata got deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91218332",
   "metadata": {},
   "source": [
    "## Types of table:\n",
    "    1. Managed \n",
    "    \n",
    "        spark.sql('CREATE TABLE my_db_spark.orders \\\n",
    "               (order_id integer, \\\n",
    "                order_date string, \\\n",
    "                customer_id integer, \\\n",
    "                order_status string) \\\n",
    "                USING csv')\n",
    "                \n",
    "        spark.sql(\"INSERT INTO orders \\\n",
    "            SELECT * \\\n",
    "            FROM orders_view\")\n",
    "            \n",
    "            \n",
    "    2. External \n",
    "    \n",
    "         spark.sql('CREATE TABLE my_db_spark.orders \\\n",
    "               (order_id integer, \\\n",
    "                order_date string, \\\n",
    "                customer_id integer, \\\n",
    "                order_status string) \\\n",
    "                USING csv \\\n",
    "                LOCATION '<S3:Path>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3b46a",
   "metadata": {},
   "source": [
    "## Create an `external` Table\n",
    "\n",
    "    - We dont own the data \n",
    "    - We own ONLY the metadata \n",
    "    - We can not delete the data (as many others might be using the data) \n",
    "        - When we drop, it will DROP only the meta data \n",
    "        - Even if we use TRUNCATE, it would fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84d8ce09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a29150b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42254f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/14 02:28:15 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS my_db_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d3e6ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "|         my_db_spark|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df444d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE my_db_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b3641f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|       my_db_spark|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.catalog.currentDatabase()\n",
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c7434c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13385af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE my_db_spark.orders \\\n",
    "           (order_id integer, \\\n",
    "            order_date string, \\\n",
    "            customer_id integer, \\\n",
    "            order_status string) \\\n",
    "            USING parquet \\\n",
    "            OPTIONS ('header'='true', 'inferSchema'='true') \\\n",
    "            LOCATION 's3://fcc-spark-example/dataset/2023/my_orders'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b5d3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM my_db_spark.orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d18a129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "#   CREATE TABLE my_db_spark.orders2 (\n",
    "#     order_id integer,\n",
    "#     order_date string,\n",
    "#     customer_id integer,\n",
    "#     order_status string\n",
    "#   )\n",
    "#   USING csv\n",
    "#   OPTIONS (\n",
    "#     'path' 's3://fcc-spark-example/dataset/2023/orders.csv',\n",
    "#     'header' 'true',\n",
    "#     'sep' ',',\n",
    "#     'inferSchema' 'true',\n",
    "#     'mode' 'FAILFAST',\n",
    "#     'quote' '\"',\n",
    "#     'escape' '\"',\n",
    "#     'multiline' 'true',\n",
    "#     'charset' 'UTF-8'\n",
    "#   )\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "158d29a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------+\n",
      "|  namespace|tableName|isTemporary|\n",
      "+-----------+---------+-----------+\n",
      "|my_db_spark|   orders|      false|\n",
      "+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3b9a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|            order_id|                 int|   null|\n",
      "|          order_date|              string|   null|\n",
      "|         customer_id|                 int|   null|\n",
      "|        order_status|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|         my_db_spark|       |\n",
      "|               Table|              orders|       |\n",
      "|               Owner|              hadoop|       |\n",
      "|        Created Time|Sun May 14 02:30:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|  Spark 3.3.0-amzn-1|       |\n",
      "|                Type|            EXTERNAL|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|s3://fcc-spark-ex...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[header=true, inf...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE EXTENDED orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bee29084",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This would FAIL \n",
    "\n",
    "# spark.sql('TRUNCATE table orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f1138",
   "metadata": {},
   "source": [
    "### DML Operations \n",
    "    - INSERT - it works -> But its mostly for OLTP application, its not for Spark ideally\n",
    "    - UPDATE - doesnt work \n",
    "    - DELETE - doesnt work \n",
    "    - SELECT - Always work \n",
    "    \n",
    "    (These are as part of Open Source Apache Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3e43f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO TABLE my_db_spark.orders VALUES (9988, '2023-05-23', 1234, 'COMPLETED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a8a43a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO TABLE my_db_spark.orders VALUES (9989, '2023-05-23', 1235, 'COMPLETED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5603fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not see any data here \n",
    "\n",
    "!hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b1f4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|    9988|2013-09-25 00:00:...|      11739|SUSPECTED_FRAUD|\n",
      "|    9989|2013-09-25 00:00:...|       4865|       COMPLETE|\n",
      "|    9989|          2023-05-23|       1235|      COMPLETED|\n",
      "|    9988|          2023-05-23|       1234|      COMPLETED|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM my_db_spark.orders WHERE order_id IN (9988, 9989)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422d55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
