{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160619b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e3ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469e304",
   "metadata": {},
   "source": [
    "# Higher Level APIs \n",
    "    - Dataframes \n",
    "    - Spark SQL \n",
    "    - Datasets -> Language specific (not available for Python, but available for Scala/Java) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d130c",
   "metadata": {},
   "source": [
    "### RDD\n",
    "- No schema \n",
    "- No metadata\n",
    "- Raw data distributed across different partitions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c2fed-08c9-44a8-9d4f-c159488c9ac0",
   "metadata": {},
   "source": [
    "![](../img/SparkArchitect.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d14a93",
   "metadata": {},
   "source": [
    "### Table\n",
    "- Consists of `data` and `metadata` \n",
    "- Data is stored at storage layer (in the form of data files)\n",
    "- Metadata is stored in some metastore which holds the schema \n",
    "- When we run `select * from table` -> It gets the checks data and metadata together to give us the data in a tabular form\n",
    "- For example: `select some_col from table` -> If this col `some_col` is not present in the metastore/table metadata, it will throw an exception. In that case it will not even look at the data files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ccf8a",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "\n",
    "- It works in a similar manner \n",
    "- Data files (S3/HDFS/etc) + Metastore (some database, admins can decide, we dont have to worrk )\n",
    "\n",
    "- Metastore on AWS\n",
    "    - When you create tables in Glue, it stores metadata about those tables (like table names, column names, data types, etc.) in the AWS Glue Data Catalog, which serves as a centralized metastore for your AWS environment. Data Catalog is integrated with Amazon S3, Amazon RDS, Amazon Redshift, and other services.\n",
    "    - If you’re using Apache Spark on Amazon EMR or other AWS environments (apart from Glue), you might choose to use the Glue Data Catalog as your metastore by configuring your Spark environment accordingly. The advantage of this is a unified metastore across multiple services and applications, all managed by AWS.\n",
    "    - This metadata is stored in a highly available and durable way, but the exact details of its storage are abstracted away from the user as part of the fully managed nature of AWS services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d55f2",
   "metadata": {},
   "source": [
    "### DataFrames and Spark SQL\n",
    "\n",
    "- Dataframes are nothing but RDD + Meradata (schema/structure)\n",
    "- `Spark Dataframes` are **not persistent (its in-memory)**\n",
    "    - Data - in-memory \n",
    "    - Metadata - in-memory \n",
    "    - There is no metastore, it is stored in a temp metadata catalog. Once the application is closed/stoped, its gone\n",
    "    - Dataframes is only visible to 1 session (our session where we create it)\n",
    "    - We can simply think of it as RDD with some structure\n",
    "    \n",
    "- `Spark Table` is always **persistent**\n",
    "    - After closing the session the data persists (data and metadata)\n",
    "    - Can be accessed via others across other sessions \n",
    "                        \n",
    "- Performance would be almost same whether we use Dataframe or Spark Table\n",
    "- They can be used interchangable, we can convert a Spark table to a Dataframe and vice-versa based on our requirement\n",
    "- Higher level APIs are more performant as Spark now knows about the metadata, and it can optimize the operation in a better and more efficient manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620897f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8dbd2",
   "metadata": {},
   "source": [
    "# Dataframe\n",
    "\n",
    "At its core, we do the following typically:\n",
    "\n",
    "    - Step 1: We load the data/some file and create a Spark DF \n",
    "    - Step 2: Perform some operation \n",
    "    - Step 3: Save/write the transformed data back to some storage (S3/HDFS/etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3ee9a",
   "metadata": {},
   "source": [
    "## Loading the data and creating a dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b8ff9",
   "metadata": {},
   "source": [
    "#### 1. CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3a118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/diamonds.csv'\n",
    "\n",
    "df = (spark.read                               # reader API\n",
    "           .format('csv')                      # format is CSV\n",
    "           .option('header', 'true')           # consider first line as header \n",
    "           .option('inferSchema', 'true')      # infer the schema automatically\n",
    "           .load(data_set)                     # load the data \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc786376-eedd-44cd-b697-56ac64e3a899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b45174-fb55-460b-a89a-ccdb3d4bfe36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9669e8a",
   "metadata": {},
   "source": [
    "> It is not prefered to use inferSchema to infer the schema\n",
    "\n",
    ">    - It may not infer the schema correctly like `datetime` column might get infered as `string`\n",
    ">    - it can lead to performance issues, as spark has to scan some data in oder to infer the schema\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200a640-9d03-4e00-b9b2-5efa45121e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (spark.read                               # reader API\n",
    "           .format('csv')                      # format is CSV\n",
    "           .option('header', 'true')           # consider first line as header \n",
    "           .option('inferSchema', 'true')      # infer the schema automatically\n",
    "           .option('samplingRatio', 0.2)       # mentioning the sampling ration of 20% \n",
    "           .load(data_set)                     # load the data \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464e384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3bf24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Another way to read the data \n",
    "\n",
    "df = (spark\n",
    "        .read\n",
    "        .csv(data_set, header=True, inferSchema=True)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95140e8",
   "metadata": {},
   "source": [
    "#### 2. JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0db2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# meta-data is embeded within the data \n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/diamonds.json'\n",
    "\n",
    "df = (spark      \n",
    "          .read   \n",
    "          .json(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d60bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb45876",
   "metadata": {},
   "source": [
    "#### 3. Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2335a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# meta-data is embeded within the data \n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/diamonds_parquet' \n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .parquet(data_set)\n",
    "      )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310db1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.repartition(4).write.format(\"parquet\").mode(\"overwrite\").save(\"s3://fcc-spark-example/dataset/diamonds_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a27e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb52bff",
   "metadata": {},
   "source": [
    "## Performing some transformations \n",
    "\n",
    "- Create a DF (Load) --> (READER API) \n",
    "- We perform some transformation \n",
    "- Store the clean/processed data (WRITER API) --> We will look later "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bfb520",
   "metadata": {},
   "source": [
    "### Change the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669286e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change the column name \n",
    "\n",
    "df2 = (df.withColumnRenamed('x', 'x_col') \n",
    "       .withColumnRenamed('y', 'y_col') \n",
    "       .withColumnRenamed('z', 'z_col') )\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa1c9e-66a5-47e2-9814-73c1d38b0fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56c5b1-98f0-45e7-bb7c-dd45944c9789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9150e5",
   "metadata": {},
   "source": [
    "### Performing some filter operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db470e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some filter operation \n",
    "\n",
    "df_premium = df.where(\"cut == 'Premium'\")\n",
    "df_premium.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ce3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# where() is an alias for filter()\n",
    "\n",
    "df_premium = df.filter(\"cut == 'Premium'\")\n",
    "df_premium.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29251907",
   "metadata": {},
   "source": [
    "### Changing the datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a84e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets create one dataframe, and later on we will change the datatype\n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "\n",
    "df_orders =  (spark.read                               # reader API\n",
    "                   .format('csv')                      # format is CSV\n",
    "                   .option('header', 'true')           # consider first line as header \n",
    "                   .option('inferSchema', 'true')      # infer the schema automatically\n",
    "                   .load(data_set)                     # load the data \n",
    "             )\n",
    "\n",
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437701f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab6465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing the data type from Integer Type to Long Type\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "df_orders2 = df_orders.withColumn('order_customer_id_2', \n",
    "                                  df_orders['order_customer_id'].cast(T.LongType())\n",
    "                                 )\n",
    "\n",
    "df_orders2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193838a-ad6d-4cdc-8415-097af4683046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_orders2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c329938",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Dataframe to Spark Table/View\n",
    "\n",
    "In Spark SQL, a table and a view both allow you to structure and organize your data, but they serve different purposes and are used differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc86c97",
   "metadata": {},
   "source": [
    "#### Creating a View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c4209-f714-4ed2-8bca-ff6003e0bd58",
   "metadata": {},
   "source": [
    "In SQL, we have two distinct concepts: \n",
    "\n",
    "- `table` is materialized in memory and on disk,\n",
    "- `view` is computed on the fly. \n",
    "\n",
    "Spark’s temp views are conceptually closer to a view than a table. \n",
    "\n",
    "A view in Spark is a logical construct. It's essentially a named SQL query that acts as a virtual table.\n",
    "\n",
    "Views do not physically store data. Instead, every time you query a view, Spark applies the view's transformation to the underlying data. This can be beneficial when you have complex transformations that you want to reuse, or when you want to simplify queries for end users.\n",
    "\n",
    "- `createOrReplaceTempView`\n",
    "- `createTempView`\n",
    "- `createOrReplaceGlobalTempView`\n",
    "- `createGlobalTempView`\n",
    "\n",
    "These transformation (e.g `createOrReplaceTempView`) will look at the data frame\n",
    "referenced by the Python variable on which the method was applied and will create a\n",
    "Spark SQL reference to the same data frame.\n",
    "\n",
    "Spark SQL also has tables as well, which we will see later. \n",
    "Tables in Spark are similar to tables in a relational database. They are data structures that organize data into rows and columns. Each column has a specific data type, and each row contains a record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1cc3-25a2-47b8-8a98-8dbe86fd5b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/diamonds_parquet' \n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .parquet(data_set)\n",
    "      )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31facb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('diamonds') \n",
    "\n",
    "# Now we have a distributed table/view called 'diamonds' in our Spark Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5e7ad-395c-4003-b1ff-268dc67e551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_premium = df.where(\"cut == 'Premium'\")\n",
    "df_premium.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e9f497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_premium = spark.sql('SELECT * \\\n",
    "                           FROM diamonds \\\n",
    "                           WHERE cut=\"Premium\" \\\n",
    "                        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046279ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_premium.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e3823-300f-48f4-a517-052302db4976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('diamonds') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9735e31-fe76-43d2-8f43-ca66b9cc0dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('diamonds2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3841b1-d4fa-4ec8-b4a4-3fd6a9878fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all the tables \n",
    "\n",
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9945807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Another way to list all the tables \n",
    "spark.catalog.listTables()\n",
    "\n",
    "# After this open a new `pyspark` shell and run the same `spark.catalog.listTables()` \n",
    "# We will see no tables, as this table was created as a \"Local Table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c6b70-3912-4b01-a32c-ab976156518b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE diamonds\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049c45f",
   "metadata": {},
   "source": [
    "#### Spark Table to Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08256b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = ( spark\n",
    "          .read\n",
    "          .table('diamonds')\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badab60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18972fb9",
   "metadata": {},
   "source": [
    "#### Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e78674-e96f-43ba-a154-ce706c58853d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19d964",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeab0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06b640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView('diamonds2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c794c7a-a831-43d5-9ef4-6358b423fb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9363f-48f5-4d72-a103-8671d9f298a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
