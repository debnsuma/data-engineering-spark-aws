{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca586d9",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31225c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596e682",
   "metadata": {},
   "source": [
    "### Check all the `databases` present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bc7dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show() # This is coming from AWS Glue Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378aa65",
   "metadata": {},
   "source": [
    "### Check all the `TABLES` present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786c98f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show() # In the default DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8edd22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE dev_feedback')   # Select a differet database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61672b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()  # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26f729",
   "metadata": {},
   "source": [
    "### Create a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197fdd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/12 20:07:31 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS my_db_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff898ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE my_db_spark') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718cc7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()  # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f900130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "|         my_db_spark|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show() # This is coming from AWS Glue Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0945d47",
   "metadata": {},
   "source": [
    "### Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52442e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_db_spark'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()    # Check the present database (which is selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "237697dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/12 20:08:00 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=beb503e2-c9f4-4a44-947a-9a8027057309, clientType=HIVECLI]\n",
      "23/05/12 20:08:00 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/05/12 20:08:00 INFO AWSCatalogMetastoreClient: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "23/05/12 20:08:00 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n",
      "23/05/12 20:08:01 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n",
      "23/05/12 20:08:01 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE TABLE my_db_spark.orders \\\n",
    "               (order_id integer, \\\n",
    "                order_date string, \\\n",
    "                customer_id integer, \\\n",
    "                order_status string)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2b07a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_db_spark'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()    # Check the present database (which is selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f888d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='orders', database='my_db_spark', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c76a94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------+\n",
      "|  namespace|tableName|isTemporary|\n",
      "+-----------+---------+-----------+\n",
      "|my_db_spark|   orders|      false|\n",
      "+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fded2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "df = spark.read.csv('s3://fcc-spark-example/dataset/2023/orders.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78edffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524f858",
   "metadata": {},
   "source": [
    "### Create a `TempView` using the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "471f2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('my_db_spark.orders_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ee60c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|  namespace|  tableName|isTemporary|\n",
      "+-----------+-----------+-----------+\n",
      "|my_db_spark|     orders|      false|\n",
      "|           |orders_view|       true|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa703bc",
   "metadata": {},
   "source": [
    "### Insert data from the `TempView` into the new `Table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cae30c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/12 20:09:22 INFO log: Updating table stats fast for orders                \n",
      "23/05/12 20:09:22 INFO log: Updated size of table orders to 2862089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO orders \\\n",
    "            SELECT * \\\n",
    "            FROM orders_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b88c7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM orders\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfb612",
   "metadata": {},
   "source": [
    "### Describe `Table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e7e5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|    order_id|      int|   null|\n",
      "|  order_date|   string|   null|\n",
      "| customer_id|      int|   null|\n",
      "|order_status|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebe2ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|            order_id|                 int|   null|\n",
      "|          order_date|              string|   null|\n",
      "|         customer_id|                 int|   null|\n",
      "|        order_status|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|         my_db_spark|       |\n",
      "|               Table|              orders|       |\n",
      "|               Owner|              hadoop|       |\n",
      "|        Created Time|Fri May 12 20:08:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|  Spark 3.3.0-amzn-1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|                hive|       |\n",
      "|          Statistics|       2862089 bytes|       |\n",
      "|            Location|hdfs://ip-172-31-...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED orders\").show() # Its a managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5198efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                       |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                                             |null   |\n",
      "|order_date                  |string                                                                                          |null   |\n",
      "|customer_id                 |int                                                                                             |null   |\n",
      "|order_status                |string                                                                                          |null   |\n",
      "|                            |                                                                                                |       |\n",
      "|# Detailed Table Information|                                                                                                |       |\n",
      "|Database                    |my_db_spark                                                                                     |       |\n",
      "|Table                       |orders                                                                                          |       |\n",
      "|Owner                       |hadoop                                                                                          |       |\n",
      "|Created Time                |Fri May 12 20:08:01 UTC 2023                                                                    |       |\n",
      "|Last Access                 |UNKNOWN                                                                                         |       |\n",
      "|Created By                  |Spark 3.3.0-amzn-1                                                                              |       |\n",
      "|Type                        |MANAGED                                                                                         |       |\n",
      "|Provider                    |hive                                                                                            |       |\n",
      "|Statistics                  |2862089 bytes                                                                                   |       |\n",
      "|Location                    |hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                              |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                                        |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                                      |       |\n",
      "|Storage Properties          |[serialization.format=1]                                                                        |       |\n",
      "+----------------------------+------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED orders\").show(truncate=False) # Its a managed table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152fae5",
   "metadata": {},
   "source": [
    "### Check the underline data in `HDFS` (Managed Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "723c3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxrwxrwt   - hadoop spark          0 2023-05-12 20:08 hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c91f698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwxrwt   1 hadoop spark    2862089 2023-05-12 20:09 hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders/part-00000-842663d4-ead0-4f34-81f7-aefb2747b906-c000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d65c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\u00012013-07-25 00:00:00\u000111599\u0001CLOSED\n",
      "2\u00012013-07-25 00:00:00\u0001256\u0001PENDING_PAYMENT\n",
      "3\u00012013-07-25 00:00:00\u000112111\u0001COMPLETE\n",
      "4\u00012013-07-25 00:00:00\u00018827\u0001CLOSED\n",
      "5\u00012013-07-25 00:00:00\u000111318\u0001COMPLETE\n",
      "6\u00012013-07-25 00:00:00\u00017130\u0001COMPLETE\n",
      "7\u00012013-07-25 00:00:00\u00014530\u0001COMPLETE\n",
      "8\u00012013-07-25 00:00:00\u00012911\u0001PROCESSING\n",
      "9\u00012013-07-25 00:00:00\u00015657\u0001PENDING_PAYMENT\n",
      "10\u00012013-07-25 00:00:00\u00015648\u0001PENDING_PAYMENT\n",
      "11\u00012013-07-25 00:00:00\u0001918\u0001PAYMENT_REVIEW\n",
      "12\u00012013-07-25 00:00:00\u00011837\u0001CLOSED\n",
      "13\u00012013-07-25 00:00:00\u00019149\u0001PENDING_PAYMENT\n",
      "14\u00012013-07-25 00:00:00\u00019842\u0001PROCESSING\n",
      "15\u00012013-07-25 00:00:00\u00012568\u0001COMPLETE\n",
      "16\u00012013-07-25 00:00:00\u00017276\u0001PENDING_PAYMENT\n",
      "17\u00012013-07-25 00:00:00\u00012667\u0001COMPLETE\n",
      "18\u00012013-07-25 00:00:00\u00011205\u0001CLOSED\n",
      "19\u00012013-07-25 00:00:00\u00019488\u0001PENDING_PAYMENT\n",
      "20\u00012013-07-25 00:00:00\u00019198\u0001PROCESSING\n",
      "21\u00012013-07-25 00:00:00\u00012711\u0001PENDING\n",
      "22\u00012013-07-25 00:00:00\u0001333\u0001COMPLETE\n",
      "23\u00012013-07-25 00:00:00\u00014367\u0001PENDING_PAYMENT\n",
      "24\u00012013-07-25 00:00:00\u000111441\u0001CLOSED\n",
      "25\u00012013-07-25 00:00:00\u00019503\u0001CLOSED\n",
      "26\u00012013-07-25 00:00:00\u00017562\u0001COMPLETE\n",
      "27\u00012013-07-25 00:00:00"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "hadoop fs -head hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders/part-00000-842663d4-ead0-4f34-81f7-aefb2747b906-c000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0967b",
   "metadata": {},
   "source": [
    "### Deleting the `table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48f8c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/12 20:10:38 INFO GlueMetastoreClientDelegate: Initiating drop table partitions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8d184be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DESCRIBE TABLE orders\").show() # It will throw an ERROR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "352ea37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It should give an error (Here we used MANAGED Table, and hence the data and metadata both got deleted when we ran DROP)\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3969981d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE my_db_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0f3efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_db_spark'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fd7d08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "997d606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
