{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55570d8a",
   "metadata": {},
   "source": [
    "# So far we have see the following:\n",
    "    - How we can create a Database\n",
    "    - How we can create a Table\n",
    "    - How to load the data from a tempView to a Table \n",
    "    - We created Managed Table, and when we dropped the table:\n",
    "        - Both the data and metadata got deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91218332",
   "metadata": {},
   "source": [
    "## Types of table:\n",
    "\n",
    "**1. Managed Table** \n",
    "\n",
    "```python\n",
    "\n",
    "# Create an empty table\n",
    "spark.sql('CREATE TABLE my_db_spark.orders \\\n",
    "       (order_id integer, \\\n",
    "        order_date string, \\\n",
    "        customer_id integer, \\\n",
    "        order_status string) \\\n",
    "        USING csv')\n",
    "\n",
    "# Load data into the table \n",
    "spark.sql(\"INSERT INTO orders \\\n",
    "    SELECT * \\\n",
    "    FROM orders_view\")\n",
    " ```   \n",
    "            \n",
    "**2. External Table** \n",
    "```python\n",
    "# No loading of data, just point to the data location \n",
    "spark.sql('CREATE TABLE my_db_spark.orders \\\n",
    "   (order_id integer, \\\n",
    "    order_date string, \\\n",
    "    customer_id integer, \\\n",
    "    order_status string) \\\n",
    "    USING csv \\\n",
    "    LOCATION '<S3:Path>') \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3b46a",
   "metadata": {},
   "source": [
    "## Create an `external` Table\n",
    "\n",
    "    - We dont own the data \n",
    "    - We own ONLY the metadata \n",
    "    - We can not delete the data (as many others might be using the data) \n",
    "        - When we drop, it will DROP only the meta data \n",
    "        - Even if we use TRUNCATE, it would fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2f84be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d8ce09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29150b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:57:07 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\n",
      "23/07/10 17:57:07 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "23/07/10 17:57:07 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42254f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:57:09 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS my_db_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3e6ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "|         my_db_spark|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df444d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE my_db_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3641f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|       my_db_spark|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.catalog.currentDatabase()\n",
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c7434c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13385af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:57:14 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=b4ddbb69-0a1f-4cda-a150-09d59c0d830b, clientType=HIVECLI]\n",
      "23/07/10 17:57:14 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/07/10 17:57:14 INFO AWSCatalogMetastoreClient: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "23/07/10 17:57:14 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n",
      "23/07/10 17:57:14 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE my_db_spark.orders \\\n",
    "           (order_id integer, \\\n",
    "            order_date string, \\\n",
    "            customer_id integer, \\\n",
    "            order_status string) \\\n",
    "            USING parquet \\\n",
    "            OPTIONS ('header'='true', 'inferSchema'='true') \\\n",
    "            LOCATION 's3://fcc-spark-example/dataset/2023/my_orders'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5d3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM orders').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18a129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are many different options available \n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#   CREATE TABLE my_db_spark.orders2 (\n",
    "#     order_id integer,\n",
    "#     order_date string,\n",
    "#     customer_id integer,\n",
    "#     order_status string\n",
    "#   )\n",
    "#   USING csv\n",
    "#   OPTIONS (\n",
    "#     'path' 's3://fcc-spark-example/dataset/2023/orders.csv',\n",
    "#     'header' 'true',\n",
    "#     'sep' ',',\n",
    "#     'inferSchema' 'true',\n",
    "#     'mode' 'FAILFAST',\n",
    "#     'quote' '\"',\n",
    "#     'escape' '\"',\n",
    "#     'multiline' 'true',\n",
    "#     'charset' 'UTF-8'\n",
    "#   )\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "158d29a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------+\n",
      "|  namespace|tableName|isTemporary|\n",
      "+-----------+---------+-----------+\n",
      "|my_db_spark|   orders|      false|\n",
      "+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b9a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|            order_id|                 int|   null|\n",
      "|          order_date|              string|   null|\n",
      "|         customer_id|                 int|   null|\n",
      "|        order_status|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|         my_db_spark|       |\n",
      "|               Table|              orders|       |\n",
      "|               Owner|              hadoop|       |\n",
      "|        Created Time|Mon Jul 10 17:57:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|  Spark 3.3.0-amzn-1|       |\n",
      "|                Type|            EXTERNAL|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|s3://fcc-spark-ex...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[header=true, inf...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE EXTENDED orders').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1c789",
   "metadata": {},
   "source": [
    "##### You can run this in a seperate shell, and we can see the data as its not an temp view\n",
    "```python\n",
    ">>> spark.sql('SHOW tables').show()\n",
    "+-----------+---------+-----------+\n",
    "|  namespace|tableName|isTemporary|\n",
    "+-----------+---------+-----------+\n",
    "|my_db_spark|   orders|      false|\n",
    "+-----------+---------+-----------+\n",
    "\n",
    ">>> \n",
    ">>> spark.sql('SELECT * FROM orders').show(5)\n",
    "+--------+--------------------+-----------+---------------+                     \n",
    "|order_id|          order_date|customer_id|   order_status|\n",
    "+--------+--------------------+-----------+---------------+\n",
    "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
    "+--------+--------------------+-----------+---------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bee29084",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This would FAIL \n",
    "\n",
    "# spark.sql('TRUNCATE table orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24485b",
   "metadata": {},
   "source": [
    "- **External Table**\n",
    "    - We ONLY own the metadata \n",
    "    - Data stays somewhere else, like here its on S3 \n",
    "    - The data may be used by others, so we dont have the rights to do anything to that data as we dont own that \n",
    "\n",
    "- **Managed Table**\n",
    "    - We OWN BOTH the metadata and data \n",
    "    - We can do whataver we want "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f1138",
   "metadata": {},
   "source": [
    "### DML Operations \n",
    "    - INSERT - it works -> But its mostly for OLTP application, its not for Spark ideally\n",
    "    - UPDATE - doesnt work  (This works in Databricks, using Delta lake)\n",
    "    - DELETE - doesnt work  (This works in Databricks, using Delta lake) \n",
    "    - SELECT - Always work \n",
    "    \n",
    "    (These are as part of Open Source Apache Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e43f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO TABLE my_db_spark.orders VALUES (9988, '2023-05-23', 1234, 'COMPLETED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a8a43a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO TABLE my_db_spark.orders VALUES (9989, '2023-05-23', 1235, 'COMPLETED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5603fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not see any data here \n",
    "\n",
    "!hadoop fs -ls hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1f4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|    9988|2013-09-25 00:00:...|      11739|SUSPECTED_FRAUD|\n",
      "|    9989|2013-09-25 00:00:...|       4865|       COMPLETE|\n",
      "|    9989|          2023-05-23|       1235|      COMPLETED|\n",
      "|    9989|          2023-05-23|       1235|      COMPLETED|\n",
      "|    9989|          2023-05-23|       1235|      COMPLETED|\n",
      "|    9988|          2023-05-23|       1234|      COMPLETED|\n",
      "|    9988|          2023-05-23|       1234|      COMPLETED|\n",
      "|    9989|          2023-05-23|       1235|      COMPLETED|\n",
      "|    9989|          2023-05-23|       1235|      COMPLETED|\n",
      "|    9988|          2023-05-23|       1234|      COMPLETED|\n",
      "|    9988|          2023-05-23|       1234|      COMPLETED|\n",
      "|    9988|          2023-05-23|       1234|      COMPLETED|\n",
      "|    9988|          2023-05-23|       1234|      COMPLETED|\n",
      "|    9989|          2023-05-23|       1235|      COMPLETED|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM my_db_spark.orders WHERE order_id IN (9988, 9989)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e841e",
   "metadata": {},
   "source": [
    "```bash \n",
    "[hadoop@ip-172-31-2-35 ~]$ aws s3 ls s3://fcc-spark-example/dataset/2023/my_orders/\n",
    "2023-07-10 17:27:22          0 _SUCCESS\n",
    "2023-07-10 17:27:07       1277 part-00000-2733a635-a8dd-4c5f-b1e1-f8789acf6330-c000.snappy.parquet\n",
    "2023-07-10 17:27:22       1277 part-00000-775b48c5-602f-45d0-8536-e559c1737bf0-c000.snappy.parquet\n",
    "[hadoop@ip-172-31-2-35 ~]$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61f537",
   "metadata": {},
   "source": [
    "# Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea39376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|       my_db_spark|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3af1e2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/10 17:57:28 INFO GlueMetastoreClientDelegate: Enabled to skip drop table partitions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e9ce3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47f1e881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE my_db_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170304e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "835220dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985822d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
