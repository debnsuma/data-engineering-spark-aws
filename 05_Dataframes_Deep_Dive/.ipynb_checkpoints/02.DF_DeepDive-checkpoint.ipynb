{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c7d777",
   "metadata": {},
   "source": [
    "# Dataframe Deep Dive (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3787618d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-35.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe3ee2f4d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7c4e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-35.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f18da0",
   "metadata": {},
   "source": [
    "## Pre-requisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ebad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 17:00:39 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\n",
      "23/06/06 17:00:39 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "23/06/06 17:00:40 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "|         my_db_spark|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database\n",
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554066db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE my_db_spark') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d09d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|       my_db_spark|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show()    # Check the present database (which is selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f7f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()  # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f40fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 17:00:44 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/06/06 17:00:44 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=01e4a922-159f-4e4a-b91e-f1160a2ecd0b, clientType=HIVECLI]\n",
      "23/06/06 17:00:44 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/06/06 17:00:44 INFO AWSCatalogMetastoreClient: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "23/06/06 17:00:44 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n",
      "23/06/06 17:00:45 INFO AWSGlueClientFactory: Using region from ec2 metadata : us-east-2\n",
      "23/06/06 17:00:45 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ip-172-31-2-35.us-east-2.compute.internal:8020/user/spark/warehouse/my_db_spark.db/orders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "+--------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a table \n",
    "spark.sql('CREATE TABLE my_db_spark.orders \\\n",
    "               (order_id integer, \\\n",
    "                order_date string, \\\n",
    "                customer_id integer, \\\n",
    "                order_status string)')\n",
    "\n",
    "spark.sql('SELECT * FROM orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90339316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 17:00:56 INFO log: Updating table stats fast for orders                \n",
      "23/06/06 17:00:56 INFO log: Updated size of table orders to 2862089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert data into the table \n",
    "\n",
    "# Step 1: Load some DF \n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "df = spark.read.csv('s3://fcc-spark-example/dataset/2023/orders.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Create a TempView \n",
    "df.createOrReplaceTempView('my_db_spark.temp_table')\n",
    "\n",
    "# Step 3: Now Insert the data to the table from the above TempView\n",
    "spark.sql(\"INSERT INTO orders \\\n",
    "            SELECT * \\\n",
    "            FROM temp_table\")\n",
    "\n",
    "# Step 4: Delete the TempView\n",
    "spark.sql(\"DROP table temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f21af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------+\n",
      "|  namespace|tableName|isTemporary|\n",
      "+-----------+---------+-----------+\n",
      "|my_db_spark|   orders|      false|\n",
      "+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()  # Show all the tables within the database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab126e9",
   "metadata": {},
   "source": [
    "So, now we have some tables which we can use to create a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fb5eb",
   "metadata": {},
   "source": [
    "## Creating Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f002dc",
   "metadata": {},
   "source": [
    "We can create a Dataframe using differ ways:\n",
    "    \n",
    "    - spark.read()\n",
    "    - spark.sql() \n",
    "    - spark.table()\n",
    "    - spark.createDataFrame() -> mostly for testing\n",
    "    - spark.range()           -> mostly for testing\n",
    "    - from an RDD using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbb2c1",
   "metadata": {},
   "source": [
    "### 1. Using `spark.read()`\n",
    "\n",
    "Reading from a file/folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40b6364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .option('header', 'true')\n",
    "           .option('inferSchema', 'true')\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ebb960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c58d73",
   "metadata": {},
   "source": [
    "### 2. Using `spark.sql()`\n",
    "\n",
    "From a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ded794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|db_youtube_analytics|\n",
      "| db_youtube_cleansed|\n",
      "|      db_youtube_raw|\n",
      "|             default|\n",
      "|        dev_feedback|\n",
      "|         my_db_spark|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68b6071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE my_db_spark') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbce6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------+\n",
      "|  namespace|tableName|isTemporary|\n",
      "+-----------+---------+-----------+\n",
      "|my_db_spark|   orders|      false|\n",
      "+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19dc4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a DF from this table \n",
    "\n",
    "df = spark.sql('SELECT * \\\n",
    "              FROM orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c25ac2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49478732",
   "metadata": {},
   "source": [
    "### 3. Using `spark.table()`\n",
    "\n",
    "From a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ff3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e3b4e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291c274",
   "metadata": {},
   "source": [
    "### 4. Using `spark.range()`\n",
    "\n",
    "Mostly for testing purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee5753a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafd44fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1, 10, 2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865e107",
   "metadata": {},
   "source": [
    "### 5. Using `spark.createDataFrame()`\n",
    "\n",
    "Again mostly for testing/development purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e110cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [ (1, \"2013-07-25 00:00:00\",2561,\"PENDING_PAYMENT\"),\n",
    "            (2, \"2013-07-25 00:00:00\",1211,\"COMPLETE\"),\n",
    "            (3, \"2013-07-25 00:00:00\",8827,\"CLOSED\"),\n",
    "            (4, \"2013-07-25 00:00:00\",1131,\"COMPLETE\"),\n",
    "            (5, \"2013-07-25 00:00:00\",1000,\"COMPLETE\") ]\n",
    "\n",
    "df = spark.createDataFrame(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2a21790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----+---------------+\n",
      "| _1|                 _2|  _3|             _4|\n",
      "+---+-------------------+----+---------------+\n",
      "|  1|2013-07-25 00:00:00|2561|PENDING_PAYMENT|\n",
      "|  2|2013-07-25 00:00:00|1211|       COMPLETE|\n",
      "|  3|2013-07-25 00:00:00|8827|         CLOSED|\n",
      "|  4|2013-07-25 00:00:00|1131|       COMPLETE|\n",
      "|  5|2013-07-25 00:00:00|1000|       COMPLETE|\n",
      "+---+-------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c970c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fcc1db",
   "metadata": {},
   "source": [
    "Lets fix the column name and enforce the data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a50cb",
   "metadata": {},
   "source": [
    "#### 1. Fixing `column names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f36c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [ (1, \"2013-07-25 00:00:00\",2561,\"PENDING_PAYMENT\"),\n",
    "            (2, \"2013-07-25 00:00:00\",1211,\"COMPLETE\"),\n",
    "            (3, \"2013-07-25 00:00:00\",8827,\"CLOSED\"),\n",
    "            (4, \"2013-07-25 00:00:00\",1131,\"COMPLETE\"),\n",
    "            (5, \"2013-07-25 00:00:00\",1000,\"COMPLETE\") ]\n",
    "\n",
    "df = spark.createDataFrame(my_data).toDF('order_id', 'order_date', 'customer_id', 'status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f25bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|         status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|       2561|PENDING_PAYMENT|\n",
      "|       2|2013-07-25 00:00:00|       1211|       COMPLETE|\n",
      "|       3|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       4|2013-07-25 00:00:00|       1131|       COMPLETE|\n",
      "|       5|2013-07-25 00:00:00|       1000|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa9d70ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----+---------------+\n",
      "|  A|                  B|   C|              D|\n",
      "+---+-------------------+----+---------------+\n",
      "|  1|2013-07-25 00:00:00|2561|PENDING_PAYMENT|\n",
      "|  2|2013-07-25 00:00:00|1211|       COMPLETE|\n",
      "|  3|2013-07-25 00:00:00|8827|         CLOSED|\n",
      "|  4|2013-07-25 00:00:00|1131|       COMPLETE|\n",
      "|  5|2013-07-25 00:00:00|1000|       COMPLETE|\n",
      "+---+-------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.toDF('A', 'B', 'C', 'D')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d436fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|         status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|       2561|PENDING_PAYMENT|\n",
      "|       2|2013-07-25 00:00:00|       1211|       COMPLETE|\n",
      "|       3|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       4|2013-07-25 00:00:00|       1131|       COMPLETE|\n",
      "|       5|2013-07-25 00:00:00|       1000|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = ['order_id', 'order_date', 'customer_id', 'status']\n",
    "df = spark.createDataFrame(my_data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4e53822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07f579",
   "metadata": {},
   "source": [
    "#### 2. Fixing `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14e537fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', LongType()))\n",
    "                 .add(StructField('order_date', StringType()))\n",
    "                 .add(StructField('order_customer_id', IntegerType()))\n",
    "                 .add(StructField('order_status', StringType()))\n",
    "                )\n",
    "\n",
    "df = spark.createDataFrame(my_data, orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "477c08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|             2561|PENDING_PAYMENT|\n",
      "|       2|2013-07-25 00:00:00|             1211|       COMPLETE|\n",
      "|       3|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       4|2013-07-25 00:00:00|             1131|       COMPLETE|\n",
      "|       5|2013-07-25 00:00:00|             1000|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44de0e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9f6e796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|             2561|PENDING_PAYMENT|\n",
      "|       2|2013-07-25 00:00:00|             1211|       COMPLETE|\n",
      "|       3|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       4|2013-07-25 00:00:00|             1131|       COMPLETE|\n",
      "|       5|2013-07-25 00:00:00|             1000|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_new = df.withColumn('order_date', F.to_timestamp(F.col('order_date')))\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ed1d064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620cfff",
   "metadata": {},
   "source": [
    "### Clean-up (Drop the `table`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bcace72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 17:01:07 INFO GlueMetastoreClientDelegate: Initiating drop table partitions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP table orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f834e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cc39b",
   "metadata": {},
   "source": [
    "### Creating a Dataframe from an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a8ac9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "rdd = sc.textFile(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f873a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,07-25-2013,11599,CLOSED',\n",
       " '2,07-25-2013,256,PENDING_PAYMENT',\n",
       " '3,07-25-2013,12111,COMPLETE',\n",
       " '4,07-25-2013,8827,CLOSED',\n",
       " '5,07-25-2013,11318,COMPLETE']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16575f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda x: (int(x.split(',')[0]),\n",
    "                   x.split(',')[1],\n",
    "                   int(x.split(',')[2]),\n",
    "                   x.split(',')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47de3fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, '07-25-2013', 11599, 'CLOSED'),\n",
       " (2, '07-25-2013', 256, 'PENDING_PAYMENT'),\n",
       " (3, '07-25-2013', 12111, 'COMPLETE'),\n",
       " (4, '07-25-2013', 8827, 'CLOSED'),\n",
       " (5, '07-25-2013', 11318, 'COMPLETE')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ecffc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', LongType()))\n",
    "                 .add(StructField('order_date', StringType()))\n",
    "                 .add(StructField('order_customer_id', IntegerType()))\n",
    "                 .add(StructField('order_status', StringType()))\n",
    "                )\n",
    "\n",
    "# One way\n",
    "df = rdd2.toDF(orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1ee42cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|07-25-2013|            11599|         CLOSED|\n",
      "|       2|07-25-2013|              256|PENDING_PAYMENT|\n",
      "|       3|07-25-2013|            12111|       COMPLETE|\n",
      "|       4|07-25-2013|             8827|         CLOSED|\n",
      "|       5|07-25-2013|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38b232ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "429ef495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|07-25-2013|            11599|         CLOSED|\n",
      "|       2|07-25-2013|              256|PENDING_PAYMENT|\n",
      "|       3|07-25-2013|            12111|       COMPLETE|\n",
      "|       4|07-25-2013|             8827|         CLOSED|\n",
      "|       5|07-25-2013|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Another way\n",
    "df = spark.createDataFrame(rdd2, orders_schema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01cf34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
