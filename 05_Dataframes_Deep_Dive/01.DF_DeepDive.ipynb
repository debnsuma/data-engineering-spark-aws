{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c7d777",
   "metadata": {},
   "source": [
    "# Dataframe Deep Dive (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3787618d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-35.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ab00ded50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7c4e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-35.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fb5eb",
   "metadata": {},
   "source": [
    "## Dataframe `Schema`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02ac4e",
   "metadata": {},
   "source": [
    "#### With inferSchema = `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40b6364",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .option('header', 'true')\n",
    "           .option('inferSchema', 'true')\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e71f3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f727d7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e4501",
   "metadata": {},
   "source": [
    "#### With `inferSchema` = `True` and `samplingRatio` = `<some ratio>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4e1c20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .option('header', 'true')\n",
    "           .option('inferSchema', 'true')\n",
    "           .option('samplingRatio', 0.01)\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0543ad8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998bfe71-f8a1-4f46-8537-345dc753c6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512061b",
   "metadata": {},
   "source": [
    "#### Enforcing the `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29609db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading WITHOUT enforcing the schema\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_1.csv'\n",
    "\n",
    "df_without_schema = (spark.read\n",
    "                           .format('csv')\n",
    "                           .load(data_set)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9189faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------------+\n",
      "|_c0|                 _c1|  _c2|            _c3|\n",
      "+---+--------------------+-----+---------------+\n",
      "|  1|2013-07-25T00:00:...|11599|         CLOSED|\n",
      "|  2|2013-07-25T00:00:...|  256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25T00:00:...|12111|       COMPLETE|\n",
      "|  4|2013-07-25T00:00:...| 8827|         CLOSED|\n",
      "|  5|2013-07-25T00:00:...|11318|       COMPLETE|\n",
      "+---+--------------------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_without_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee452ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_without_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911235a2",
   "metadata": {},
   "source": [
    "There are 2 ways we can define/enforce the `schema`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937d5f3",
   "metadata": {},
   "source": [
    "#### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac36dc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DDL Style \n",
    "orders_schema = 'order_id long, order_date timestamp, order_customer_id long, order_status string'\n",
    "\n",
    "df_with_schema = (spark.read\n",
    "                       .format('csv')\n",
    "                       .schema(orders_schema)\n",
    "                       .load(data_set)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319cb66c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b5f36d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7d8339f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What if we load some schema which doesnt match with the data \n",
    "# It will full the columns with NULLs \n",
    "\n",
    "\n",
    "# Lets make the `order_status` as LONG \n",
    "orders_schema = 'order_id long, order_date timestamp, order_customer_id long, order_status long'\n",
    "\n",
    "df_with_schema = (spark.read\n",
    "                           .format('csv')\n",
    "                           .schema(orders_schema)\n",
    "                           .load(data_set)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0060d9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+------------+\n",
      "|order_id|         order_date|order_customer_id|order_status|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|        null|\n",
      "|       2|2013-07-25 00:00:00|              256|        null|\n",
      "|       3|2013-07-25 00:00:00|            12111|        null|\n",
      "|       4|2013-07-25 00:00:00|             8827|        null|\n",
      "|       5|2013-07-25 00:00:00|            11318|        null|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b911802-6eb5-4a1e-a390-52b6eb7de000",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_status: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313b749",
   "metadata": {},
   "source": [
    "#### Method 2 (using `StructType`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb81426e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_1.csv'\n",
    "\n",
    "orders_schema = StructType([\n",
    "                            StructField('order_id', T.LongType()),\n",
    "                            StructField('order_date', T.DateType()),\n",
    "                            StructField('order_customer_id', T.IntegerType()),\n",
    "                            StructField('order_status', T.StringType())\n",
    "                        ])\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49386065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d87226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `Date` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfbf84-a3db-495c-9ce2-2ad8ba3feba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First lets look into 'orders_1' and 'orders_2' file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b58ececb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.DateType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbc55044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This will error out  (ERROR: The value '07-25-2013' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2104e",
   "metadata": {},
   "source": [
    "Two ways to deal with this:\n",
    "- Load using `String` and later on change it \n",
    "- Somehow inform Spark about the exact format of the date schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca2c60",
   "metadata": {},
   "source": [
    "#### 1. Load using `String`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5207cbab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "# Step 1 : Load using StringType\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "# Step 2 : Perform the transformation to convert the data into the right format \n",
    "df_transformed = df.withColumn('order_date', F.to_date(F.col('order_date'), \"MM-dd-yyyy\"))\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b32e6c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a8209c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|07-25-2013|            11599|         CLOSED|\n",
      "|       2|07-25-2013|              256|PENDING_PAYMENT|\n",
      "|       3|07-25-2013|            12111|       COMPLETE|\n",
      "|       4|07-25-2013|             8827|         CLOSED|\n",
      "|       5|07-25-2013|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Old `df` with string dataType (before the step 2) \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2942a2a-4b4d-4d2c-bc2d-9bb5b8e6797c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "805da157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|      null|            11599|         CLOSED|\n",
      "|       2|      null|              256|PENDING_PAYMENT|\n",
      "|       3|      null|            12111|       COMPLETE|\n",
      "|       4|      null|             8827|         CLOSED|\n",
      "|       5|      null|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# If we parse WRONG format \n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "# Step 1 : Load using StringType\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "# Step 2 : Perform the transformation to convert the date with WRONG format \n",
    "df_transformed = df.withColumn('order_date', F.to_date(F.col('order_date'), \"dd-MM-yyyy\"))\n",
    "\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c6f34",
   "metadata": {},
   "source": [
    "#### 2. Loading using `dateFormat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "592eae4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.DateType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('dateFormat', 'MM-dd-yyyy')          # using the dateFormat\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f0892",
   "metadata": {},
   "source": [
    "#### Similarly, if the column has integers and string both, and we load it as a `IntegerType()`, we will get the data as `NULL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab11a97c-b1cf-42af-9129-cb32e300782f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       2|2013-07-25|            12111|       COMPLETE|\n",
      "|       3|2013-07-25|             null|         CLOSED|\n",
      "|       4|2013-07-25|            11318|       COMPLETE|\n",
      "|       5|2013-07-25|             null|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.DateType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('dateFormat', 'M/dd/yyyy')          \n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d6323-18f9-471a-a243-5cb7689271bf",
   "metadata": {},
   "source": [
    "#### We can change this behaviour and we will see next.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677567ef-09a6-4734-972d-e2ab6c61b948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f8863-46bd-4c11-b095-0efddfe98644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86e65e-2e58-4e87-b4b8-f9b7907e795f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4eb484-ff99-4707-93b3-84a973d38d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796aad27-3b33-413c-b078-d13ceed65f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459e9f2-bc73-444e-991f-7e716eca3072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b46da-522c-42d8-9468-b79bde958ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e602219-5e8e-4aa7-bf9b-30a8a2ee9ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc23e83-1e8f-404c-a128-39642058926d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e37a35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modes of reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3b2d4",
   "metadata": {},
   "source": [
    "When reading data with Spark, there are different modes available to handle corrupt or malformed records encountered during the read process. These modes determine how Spark should behave when it encounters such records.\n",
    "\n",
    "- **Permissive mode:** [`DEFAULT`] Permissive mode (mode=`permissive`, which is the default) allows Spark to continue reading the data even if it encounters corrupt or malformed records. When a corrupt record is encountered, Spark tries to parse and load as much data as possible. It inserts `null` or `NaN values` for the corrupt fields and includes the malformed records in the resulting DataFrame. This mode is helpful when you want to handle corrupt records separately or perform additional error handling.\n",
    "\n",
    "- **Failfast mode:** In this mode (mode=`failfast`), Spark fails immediately upon encountering any corrupt or malformed record. It throws an exception and stops the read operation. No data is returned. This mode is useful when you want to ensure data integrity and immediately identify any issues with the data.\n",
    "\n",
    "- **Dropmalformed mode:** Dropmalformed mode (mode=`dropmalformed`) instructs Spark to drop any records that cannot be parsed correctly. When a malformed record is encountered, Spark excludes it from the resulting DataFrame entirely. This mode is useful when you want to discard any records that do not conform to the expected schema or format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63b948",
   "metadata": {},
   "source": [
    "#### Permissive mode (`default` mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a23fd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv/'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))       # Although this column contains string\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b3b9fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       2|2013-07-25|            12111|       COMPLETE|\n",
      "|       3|2013-07-25|             null|         CLOSED|\n",
      "|       4|2013-07-25|            11318|       COMPLETE|\n",
      "|       5|2013-07-25|             null|       COMPLETE|\n",
      "|       6|2013-07-25|             4530|       COMPLETE|\n",
      "|       7|2013-07-25|             2911|     PROCESSING|\n",
      "|       8|2013-07-25|             5657|PENDING_PAYMENT|\n",
      "|       9|2013-07-25|             null|PENDING_PAYMENT|\n",
      "|      10|2013-07-24|              918| PAYMENT_REVIEW|\n",
      "|      11|2013-07-24|             1837|         CLOSED|\n",
      "|      12|2013-07-24|             9149|PENDING_PAYMENT|\n",
      "|      13|2013-07-24|             9842|     PROCESSING|\n",
      "|      14|2013-07-24|             null|       COMPLETE|\n",
      "|      15|2013-07-24|             7276|PENDING_PAYMENT|\n",
      "|      16|2013-07-24|             2667|       COMPLETE|\n",
      "|      17|2013-07-24|             1205|         CLOSED|\n",
      "|      18|2013-07-24|             9488|PENDING_PAYMENT|\n",
      "|      19|2013-07-24|             9198|     PROCESSING|\n",
      "+--------+----------+-----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e8d3c",
   "metadata": {},
   "source": [
    "#### Failfast mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e115557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv/'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('mode', 'failfast')\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c808a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This will error out\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f02cd7",
   "metadata": {},
   "source": [
    "#### Dropmalformed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca5f828d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       2|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|            11318|       COMPLETE|\n",
      "|       6|2013-07-25|             4530|       COMPLETE|\n",
      "|       7|2013-07-25|             2911|     PROCESSING|\n",
      "|       8|2013-07-25|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-24|              918| PAYMENT_REVIEW|\n",
      "|      11|2013-07-24|             1837|         CLOSED|\n",
      "|      12|2013-07-24|             9149|PENDING_PAYMENT|\n",
      "|      13|2013-07-24|             9842|     PROCESSING|\n",
      "|      15|2013-07-24|             7276|PENDING_PAYMENT|\n",
      "|      16|2013-07-24|             2667|       COMPLETE|\n",
      "|      17|2013-07-24|             1205|         CLOSED|\n",
      "|      18|2013-07-24|             9488|PENDING_PAYMENT|\n",
      "|      19|2013-07-24|             9198|     PROCESSING|\n",
      "+--------+----------+-----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv/'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('mode', 'dropmalformed')\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0a092-2ba3-4e87-aa24-65ccf0893da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
