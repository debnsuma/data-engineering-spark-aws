{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c7d777",
   "metadata": {},
   "source": [
    "# Dataframe Deep Dive (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787618d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c4e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fb5eb",
   "metadata": {},
   "source": [
    "## Dataframe `Schema`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02ac4e",
   "metadata": {},
   "source": [
    "#### With inferSchema = `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b6364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .option('header', 'true')\n",
    "           .option('inferSchema', 'true')\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71f3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727d7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e4501",
   "metadata": {},
   "source": [
    "#### With `inferSchema` = `True` and `samplingRatio` = `<some ratio>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e1c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders.csv'\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .option('header', 'true')\n",
    "           .option('inferSchema', 'true')\n",
    "           .option('samplingRatio', 0.01)\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543ad8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bfe71-f8a1-4f46-8537-345dc753c6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512061b",
   "metadata": {},
   "source": [
    "#### Enforcing the `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc2260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29609db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_without_schema = (spark.read\n",
    "                           .format('csv')\n",
    "                           .load(data_set)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9189faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_without_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee452ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_without_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911235a2",
   "metadata": {},
   "source": [
    "There are 2 ways we can define/enforce the `schema`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937d5f3",
   "metadata": {},
   "source": [
    "#### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36dc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date timestamp, order_customer_id long, order_status string'\n",
    "\n",
    "df_with_schema = (spark.read\n",
    "                           .format('csv')\n",
    "                           .schema(orders_schema)\n",
    "                           .load(data_set)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319cb66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5f36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8339f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What if we load some schema which doesnt match with the data \n",
    "# It will full the columns with NULLs \n",
    "\n",
    "\n",
    "# making order_status as LONG \n",
    "orders_schema = 'order_id long, order_date timestamp, order_customer_id long, order_status long'\n",
    "\n",
    "df_with_schema = (spark.read\n",
    "                           .format('csv')\n",
    "                           .schema(orders_schema)\n",
    "                           .load(data_set)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060d9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b911802-6eb5-4a1e-a390-52b6eb7de000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313b749",
   "metadata": {},
   "source": [
    "#### Method 2 (using `StructType`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81426e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_1.csv'\n",
    "\n",
    "orders_schema = StructType([\n",
    "                            StructField('order_id', T.LongType()),\n",
    "                            StructField('order_date', T.DateType()),\n",
    "                            StructField('order_customer_id', T.IntegerType()),\n",
    "                            StructField('order_status', T.StringType())\n",
    "                        ])\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49386065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac579c9-1f71-4c2a-bd42-09b8b688f976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets try to using `IntergerType` for the `order_status` column \n",
    "\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_1.csv'\n",
    "\n",
    "orders_schema = StructType([\n",
    "                            StructField('order_id', T.LongType()),\n",
    "                            StructField('order_date', T.DateType()),\n",
    "                            StructField('order_customer_id', T.IntegerType()),\n",
    "                            StructField('order_status', T.IntegerType())\n",
    "                        ])\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95086de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_1.csv'\n",
    "\n",
    "orders_schema = StructType([\n",
    "                            StructField('order_id', T.LongType(), False),\n",
    "                            StructField('order_date', T.DateType(), False),\n",
    "                            StructField('order_customer_id', T.IntegerType(), False),\n",
    "                            StructField('order_status', T.StringType(), False)\n",
    "                        ])\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8cfc1",
   "metadata": {},
   "source": [
    "In PySpark, the `nullable` parameter in the schema definition using `StructType` **does not enforce** `non-nullability` in the DataFrame. It is primarily used as a hint for optimization purposes and does not restrict the presence of null values in the columns.\n",
    "\n",
    "Even if you set `nullable=False` for the fields in the schema definition, **it does not guarantee that the corresponding columns in the DataFrame will not contain null values.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c6c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets see the example \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_1.csv'\n",
    "\n",
    "orders_schema = StructType([\n",
    "                            StructField('order_id', T.LongType(), False),\n",
    "                            StructField('order_date', T.DateType(), False),\n",
    "                            StructField('order_customer_id', T.IntegerType(), False),\n",
    "                            StructField('order_status', T.StringType(), False)\n",
    "                        ])\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "\n",
    "df_filtered = df.filter(F.col('order_date') \\\n",
    "                .isNull())\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d87226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `Date` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfbf84-a3db-495c-9ce2-2ad8ba3feba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First lets look into 'orders_1' and 'orders_2' file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ececb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.DateType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc55044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This will error out  \n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2104e",
   "metadata": {},
   "source": [
    "Two ways to deal with this:\n",
    "- Load using `String` and later on change it \n",
    "- Somehow inform Spark about the exact format of the date schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca2c60",
   "metadata": {},
   "source": [
    "#### 1. Load using `String`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207cbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "# Step 1 : Load using StringType\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "# Step 2 : Perform the transformation to convert the data into the right format \n",
    "df_transformed = df.withColumn('order_date', F.to_date(F.col('order_date'), \"MM-dd-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e6c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8209c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old `df` with string dataType (before the step 2) \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805da157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If we parse WRONG format \n",
    "\n",
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "# Step 1 : Load using StringType\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "# Step 2 : Perform the transformation to convert the date with WRONG format \n",
    "df_transformed = df.withColumn('order_date', F.to_date(F.col('order_date'), \"dd-MM-yyyy\"))\n",
    "\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c6f34",
   "metadata": {},
   "source": [
    "#### 2. Loading using `dateFormat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592eae4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_2.csv'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.DateType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('dateFormat', 'MM-dd-yyyy')          # using the dateFormat\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f0892",
   "metadata": {},
   "source": [
    "#### Similarly if the column has integers and string, and we load it as a `IntegerType()`, we will get the data as `NULL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11a97c-b1cf-42af-9129-cb32e300782f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.DateType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('dateFormat', 'M/dd/yyyy')          \n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d6323-18f9-471a-a243-5cb7689271bf",
   "metadata": {},
   "source": [
    "#### We can change this behaviour and we will see next.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7523d-2957-477c-a9c8-dfbabb21d661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580af49-4a1e-435f-b2ed-04e976aea6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3cf09-4fa9-4ff9-8c18-3d5e151f872b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b46da-522c-42d8-9468-b79bde958ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e602219-5e8e-4aa7-bf9b-30a8a2ee9ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc23e83-1e8f-404c-a128-39642058926d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e37a35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modes of reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3b2d4",
   "metadata": {},
   "source": [
    "When reading data with Spark, there are different modes available to handle corrupt or malformed records encountered during the read process. These modes determine how Spark should behave when it encounters such records.\n",
    "\n",
    "- **Permissive mode:** [`DEFAULT`] Permissive mode (mode=`permissive`, which is the default) allows Spark to continue reading the data even if it encounters corrupt or malformed records. When a corrupt record is encountered, Spark tries to parse and load as much data as possible. It inserts `null` or `NaN values` for the corrupt fields and includes the malformed records in the resulting DataFrame. This mode is helpful when you want to handle corrupt records separately or perform additional error handling.\n",
    "\n",
    "- **Failfast mode:** In this mode (mode=`failfast`), Spark fails immediately upon encountering any corrupt or malformed record. It throws an exception and stops the read operation. No data is returned. This mode is useful when you want to ensure data integrity and immediately identify any issues with the data.\n",
    "\n",
    "- **Dropmalformed mode:** Dropmalformed mode (mode=`dropmalformed`) instructs Spark to drop any records that cannot be parsed correctly. When a malformed record is encountered, Spark excludes it from the resulting DataFrame entirely. This mode is useful when you want to discard any records that do not conform to the expected schema or format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63b948",
   "metadata": {},
   "source": [
    "#### Permissive mode (`default` mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23fd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv/'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b9fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e8d3c",
   "metadata": {},
   "source": [
    "#### Failfast mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e115557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv/'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('mode', 'failfast')\n",
    "           .load(data_set)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c808a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This will error out\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f02cd7",
   "metadata": {},
   "source": [
    "#### Dropmalformed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f828d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set = 's3://fcc-spark-example/dataset/2023/orders/orders_3.csv/'\n",
    "\n",
    "orders_schema = (StructType()\n",
    "                 .add(StructField('order_id', T.LongType()))\n",
    "                 .add(StructField('order_date', T.StringType()))\n",
    "                 .add(StructField('order_customer_id', T.IntegerType()))\n",
    "                 .add(StructField('order_status', T.StringType()))\n",
    "                )\n",
    "\n",
    "df = (spark.read\n",
    "           .format('csv')\n",
    "           .schema(orders_schema)\n",
    "           .option('mode', 'dropmalformed')\n",
    "           .load(data_set)\n",
    "     )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0a092-2ba3-4e87-aa24-65ccf0893da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
