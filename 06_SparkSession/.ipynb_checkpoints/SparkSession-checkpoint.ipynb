{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e4885c",
   "metadata": {},
   "source": [
    "# Introduction to Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4987bd1",
   "metadata": {},
   "source": [
    "- Its an entry point to the Spark Cluster for **higher level API** like:\n",
    "\n",
    "    - Dataframe\n",
    "    - Spark SQL \n",
    "    \n",
    "- And when we need to work with **lower level API** like RDD, we need Spark Context\n",
    "\n",
    "- Instead of having different context like \n",
    "\n",
    "    - Spark Context \n",
    "    - Hive Context\n",
    "    - SQL Context, etc. \n",
    "\n",
    "- We can create a `SparkSession` which encapsulates all context under one single `SparkSession`\n",
    "\n",
    "- We should ideally have 1 SparkSession per Application \n",
    "\n",
    "- We can have multiple SparkSession\n",
    "\n",
    "- But within an Application we can have only one SparkContext which would be shared across multiple SparkSession \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700d16e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# # Create a SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Spark Session 1\") \\\n",
    "#     .config(\"spark.executor.memory\", \"4g\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .config(\"spark.sql.warehouse.dir\", \"s3://your_bucket/your_warehouse_dir\") \\\n",
    "#     .config(\"spark.master\", \"yarn\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Verify SparkSession\n",
    "# print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3606f45",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
       " ('spark.sql.parquet.output.committer.class',\n",
       "  'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'),\n",
       " ('spark.blacklist.decommissioning.timeout', '1h'),\n",
       " ('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1682703649584_0134'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://ip-172-31-2-35.us-east-2.compute.internal:20888/proxy/application_1682703649584_0134'),\n",
       " ('spark.sql.emr.internal.extensions',\n",
       "  'com.amazonaws.emr.spark.EmrSparkSessionExtensions'),\n",
       " ('spark.eventLog.dir', 'hdfs:///var/log/spark/apps'),\n",
       " ('spark.sql.warehouse.dir', 'hdfs:///user/spark/warehouse'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.executor.memory', '4743M'),\n",
       " ('spark.app.startTime', '1686150389003'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'),\n",
       " ('spark.emr.default.executor.memory', '4743M'),\n",
       " ('spark.hadoop.yarn.timeline-service.enabled', 'false'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
       " ('spark.driver.memory', '2048M'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip:/usr/lib/spark/python/:/home/hadoop/.local/lib/python3.7/site-packages::/usr/lib/spark/python:/usr/lib/spark/python:/usr/lib/spark/python<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.5-src.zip'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://ip-172-31-2-35.us-east-2.compute.internal:4044'),\n",
       " ('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true'),\n",
       " ('spark.app.id', 'application_1682703649584_0134'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.decommissioning.timeout.threshold', '20'),\n",
       " ('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true'),\n",
       " ('spark.driver.host', 'ip-172-31-2-35.us-east-2.compute.internal'),\n",
       " ('spark.yarn.historyServer.address',\n",
       "  'ip-172-31-2-35.us-east-2.compute.internal:18080'),\n",
       " ('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem',\n",
       "  '2'),\n",
       " ('spark.yarn.dist.files',\n",
       "  'file:/etc/spark/conf.dist/hive-site.xml,file:/etc/hudi/conf.dist/hudi-defaults.conf'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.sql.hive.metastore.sharedPrefixes',\n",
       "  'com.amazonaws.services.dynamodbv2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem',\n",
       "  'true'),\n",
       " ('spark.executor.extraClassPath',\n",
       "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
       " ('spark.resourceManager.cleanupExpiredHost', 'true'),\n",
       " ('spark.executor.defaultJavaOptions',\n",
       "  \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
       " ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'ip-172-31-2-35.us-east-2.compute.internal'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.app.submitTime', '1686150388572'),\n",
       " ('spark.driver.port', '43933'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.emr.default.executor.cores', '4'),\n",
       " ('spark.blacklist.decommissioning.enabled', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all the configuration details \n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288abfa",
   "metadata": {},
   "source": [
    "# Deployment Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d48707",
   "metadata": {},
   "source": [
    "- Each Spark Application will have :\n",
    "    \n",
    "    - One Driver (Master) \n",
    "    - Multiple Executors (Workers)\n",
    "\n",
    "- If driver goes down, the applition will go down \n",
    "\n",
    "- So, we can run/deploy the application in 2 modes: \n",
    "\n",
    "    - **Client Mode** - Driver runs at the client node \n",
    "    - **Cluster Mode** - Driver runs in one of the worker node (in the cluster itself) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b12035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
