Introduction to Dataframes
==========================

RDD 
===
- Raw data distributed across different partitions 
- No schema or metadata attached to it 

Table 
=====
- It has 2 parts 
    - Data --> stored in the form of some file in the storage layer 
    - Metadata --> It contains the schema of the data (it is stored in some metastore) 

- SELECT * FROM <table_name>
- SELECT id from orders;

Spark SQL 
========

- it also works in a similar way 
- Data Store (Storage Layer) - HDFS, S3, etc. 
- Metadata - It could be some database

Dataframes and Spark SQL/Table 
==============================
- Dataframe as RDD + Metadata/Schema 
- Dataframe are not persistent (its in-memory)

- Spark Table 
- It is persistent 

- Created a DB 
    - Created a Table 
    - In this table (orders), we need to insert some data, how we can do that ? 
        - Create a DF 
        - Create a tempView 
        - We will this tempView to insert data into the newly created table (orders)

==================
Spark Optimization
==================

1. Application level optimization 
    - code optimization 
    - use of cache 
    - use of reduceByKey in place of groupByKey 

2. Cluster level optimization 
    - how spark execute our code 
    - how those resources are allocated accross the executors (containers/JVM)

Executors 
=========

Right amount of resources - for our JOBs 

Resources:
    - Memory 
    - CPU 

Container/JVM/Executors (all are same from SPARK standpoint)

1 Node/Server/Worker Node 
1, 2, 3... 

Worker Node 1
-------------
64GB RAM 
16   CPU Cores 

1 CPU Core and 1 GB of Memory is reserved for background processes (deamon threads)
============
63 GB of RAM 
15 CPU Core 
============

Two different strategies 

1. Thin Executor 
    - intention is to create max no. of executors which is holding min possible resources 
    15 executors, each having 1 CPU core 
    63/15 GB of memory 

2. Fat Executor 
    - intention is to create max amount of resources to each executor 
    1 Executor with 63GB RAM and 15 CPU Cores 


How can we go ahead with our Executors 
=======================================

63GB of RAM 
15 CPU Cores 

3 Executors 
each having 5 CPU cores and 63/3 ~ 21 GB of RAM 

Each Executor wont be able to use the entire 21 GB of RAM 
    - Off Heap Memory/Overhead = max(384MB, 7% of the executor memory) = max(384MB, 1.5GB)
    - left with 21 - 1.5 ~ 19.5GB of RAM 
    